# Защита данных клиентов страховой компании

## Цели и задачи проекта

- Необходимо защитить данные клиентов страховой компании «Хоть потоп».
- Разработайте такой метод преобразования данных, чтобы по ним было сложно восстановить персональную информацию.
- Обоснуйте корректность его работы.
- Нужно защитить данные, чтобы при преобразовании качество моделей машинного обучения не ухудшилось.

## Данные

- Пол
- Возраст
- Зарплата
- Члены семьи
- Страховые выплаты

## Использованные библиотеки

- pandas
- numpy
- sklearn (LinearRegression, r2_score)

## Навыки

- линейная алгебра

## Вывод

**Алгоритм**

- [x] Выделяем из датафрейма `features`
- [x] Создаем рандомную квадратную матрицу `key martix` размером = `кол-во столбцов в датафрейме - 1` ("-1" потому что данные без целевого признака)
- [x] Убеждаемся, что `key martix` является обратимой при помощи `np.linalg.det()`
- [x] Шифруем данные = `features.values` @ `key martix`
- [x] Для обратного преобразования зашифрованных данных - `шифрованные данные` @ `np.linalg.inv(key_matrix)` (обратную ключевую матрицу)

**Обоснование**

- Подобный способ хорошо шифрует данные (подобрать ключевую матрицу очень сложно)
- Умножение матриц не должно сильно повлиять качество обучения модели потому что данные умножаются на одни те же числа из ключевой матрицы

Если: Матрица признаков - X Обратимая матрица - P Вектор целевого признака - y Вектор весов целевого признака - w

Предсказания в линейной регрессии:

$$a = X w$$

Формула вектора весов линейной регрессии:

$$w = (X^TX)^{-1}X^Ty$$

Преобразуем выражение для $w'$ - с зашифрованными признаками:

$$    w' = (Y^TY)^{-1} Y^Ty$$
$$    w'= ((XP)^T(XP))^{-1} (XP)^Ty$$
$$    w'= P^{-1}((XP)^{T}X)^{-1} (XP)^Ty$$
$$    w'= P^{-1}((XP)^{T}X)^{-1} P^TX^Ty$$
$$    w'= P^{-1}(P^TX^TX)^{-1} P^TX^Ty$$
$$   w'= P^{-1}(X^TX)^{-1}(P^T)^{-1} P^TX^Ty$$

Так как $(P^{T})^{-1} P^T = E$, а при умножениия матрицы на единичную, мтарица остается неизменной, можем просто убрать эту часть из уравнения.

Получится:

$$   w'= P^{-1}(X^TX)^{-1}X^Ty$$

Тогда заменим $(X^TX)^{-1}X^Ty$ на $w$:

$$w'= P^{-1}w$$

Подставим для $X'$ и $w'$ в формулу для расчета $a'$ - предсказания по зашифрованным признакам:

$$a'=X'w' = XPP^{-1}w = Xw = a$$

Выражения тождественны. Векторы предсказаний совпали.

- Метод шифрования данных работает
- Шифрование никак не влияет на качество обучаемой модели (значение предсказания $a$ не меняется, если умножать матрицу признаков на обратимую матрицу.)